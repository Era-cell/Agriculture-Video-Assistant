{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339139a-913c-4015-8a7f-76a4c1bbf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "img_width, img_height = 224, 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be77e4-d7a1-4e5a-b450-1f2e183f8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"path/to/data\"\n",
    "count=0\n",
    "for name in os.listdir(directory):\n",
    "    count+=len([f for f in os.listdir(os.path.join(directory,name)) if os.path.isfile(os.path.join(os.path.join(directory,name), f))])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da7e7a-0f4b-4f81-afdd-88e336bac78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"path/to/data\"\n",
    "validation_data_dir = \"path/to/data\"\n",
    "nb_train_samples =count\n",
    "nb_validation_samples = count\n",
    "epochs = 20\n",
    "batch_size = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931432e6-9c39-455a-8c31-a33a0b73d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "\tinput_shape = (3, img_width, img_height)\n",
    "else:\n",
    "\tinput_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab599b0-65b8-459f-940a-3b263a17f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (4, 4), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6efe1-fc05-479b-8c7a-8f1fd1d5bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee949d16-b9ff-4668-82b8-13cdd4590470",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a28deb-9f6c-46e1-9082-8cd938263ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_saved4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec90480-3774-4a71-a8e6-f2b7d8addd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_saved3.h5')\n",
    "\n",
    "image = load_img(\"path/to/data (3)/rice_plant_lacks_nutrients/Nitrogen(N)/untitled-94.JPG\", target_size=(224, 224))\n",
    "img = np.array(image)\n",
    "img = img / 255.0\n",
    "img = img.reshape(1,224,224,3)\n",
    "label = model.predict(img)\n",
    "maxIndex=-1\n",
    "a=label[0]\n",
    "i=0\n",
    "import numpy as np\n",
    "i=np.argmax(a)\n",
    "if i==0:\n",
    "    print(\"class is potassium\")\n",
    "elif i==1:\n",
    "    print(\"class is phosphorous\")\n",
    "else:\n",
    "    print(\"class is nitrogen\")\n",
    "#print(\"Predicted Class (0 - Cars , 1- Planes): \", label[0][0])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395a37b-fe95-4d2f-b37e-65a145d87600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "doc=\"\"\n",
    "model = load_model('model_saved4.h5')\n",
    "def predict_image(img):\n",
    "    \n",
    "    image=load_img(img,target_size=(224,224))\n",
    "    img1 = np.array(image)\n",
    "    img1 = img1 / 255.0\n",
    "    img1 = img1.reshape(1,224,224,3)\n",
    "    label = model.predict(img1)\n",
    "    a=label[0]\n",
    "    i=0\n",
    "    i=np.argmax(a)\n",
    "    if i==0:\n",
    "     return \"1\"\n",
    "    elif i==1:\n",
    "     return \"2\"\n",
    "    elif i==2:\n",
    "     return \"3\"\n",
    "    elif i==3:\n",
    "     return \"4\"\n",
    "    elif i==4:\n",
    "     return \"5\"\n",
    "    elif i==5:\n",
    "     return \"6\"\n",
    "    else :\n",
    "     return \"7\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068f5b6-4eaf-4f5c-bd13-74960839b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings,OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import AzureOpenAI\n",
    "import constants\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "def botmessage(query,cl):\n",
    " doc=\"\"\n",
    " if cl==\"3\":\n",
    "     doc=\"path/to/data/docs/Phosporous deficiency in rice.pdf\"\n",
    " elif cl==\"4\":\n",
    "     doc=\"path/to/data/docs/Potasium deficiency in rice.pdf\"\n",
    " #elif cl==\"3\":\n",
    "     doc=\"path/to/data/docs/Nitrogen deficiency in rice.pdf\"\n",
    " elif cl==\"2\":\n",
    "     doc=\"path/to/data/docs/Apple_Black_rot.pdf\"\n",
    " #elif cl==\"5\":\n",
    "     doc=\"path/to/data/docs/Apple_Cedar_rust.pdf\"\n",
    " elif cl==\"1\":\n",
    "     doc=\"path/to/data/docs/apple_scab.pdf\"\n",
    " loader = PyPDFLoader(doc)\n",
    " documents=loader.load()\n",
    " text_splitter=CharacterTextSplitter(chunk_size=3000,chunk_overlap=0)\n",
    " texts=text_splitter.split_documents(documents)\n",
    " embeddings=OpenAIEmbeddings()\n",
    " search=Chroma.from_documents(texts,embeddings)\n",
    " chain=RetrievalQA.from_chain_type(llm=ChatOpenAI(),chain_type='stuff',retriever=search.as_retriever())\n",
    " return chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178adae-7c51-498a-8cab-8a563b3ade56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "with gr.Blocks() as demo:\n",
    " image = gr.Image(type= 'filepath',height=300,width=300)\n",
    " msg = gr.Textbox(visible=False)\n",
    " #submit_button1 = gr.Button(\"Submit\")\n",
    " #submit_button1.click(fn=predict_image,inputs=image,outputs=image)\n",
    " gr.Interface(fn=predict_image,inputs=image,outputs=msg)\n",
    " chatbot = gr.Chatbot()\n",
    " \n",
    " \n",
    " msg2=gr.Textbox()\n",
    " #submit_button2 = gr.Button(\"Submit\")\n",
    " clear = gr.ClearButton([msg2, chatbot])\n",
    " def respond(message,cl, chat_history):\n",
    "        bot_message = botmessage(query=message,cl=cl)\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    " \n",
    " msg2.submit(respond, [msg2,msg, chatbot], [msg2, chatbot])\n",
    " \n",
    " #image.submit(respond)\n",
    "if __name__==\"__main__\":\n",
    "  demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39805b5-aae2-4451-ae6e-d0d9e7cbdb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(\"path/to/data/apple/Apple___Black_rot/ff4e792b-14c5-41db-af9e-0676abf8d20a___JR_FrgE.S 3088_new30degFlipLR.JPG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a249ef2-9eb3-4ac8-9eed-c546180690a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7511608-136e-41b5-a8d2-9da7822ee3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc6fa2-bdd6-460c-b84b-4b5ead597372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
